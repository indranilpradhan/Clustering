{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given a dataset of documents with content from 5 different fields. To cluster the documents  I have used KMeans algorithm where k = 5 and iteration is used to converge is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To measure euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x,c):\n",
    "    sqrsum = 0\n",
    "    for i in range(len(x)):\n",
    "        sqrsum = sqrsum + ((x[i] - c[i]) ** 2)\n",
    "    return math.sqrt(sqrsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data three fields have been used.\n",
    "\"text\" - The content of the documents is stored in this field\n",
    "\"filename\" - The name of the file is stored in this field\n",
    "\"label\" - The label of the documents is stored in this field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['filename','text', 'label']) \n",
    "base_dir = \"/media/indranil/New Volume/second sem/SMAI/Assignment 2/q6/data/dataset/\"\n",
    "for filename in os.listdir(base_dir):\n",
    "    path = os.path.join(base_dir, filename)\n",
    "    with open(path, \"r\", encoding='latin1') as file:\n",
    "        text = file.read()\n",
    "        label = filename[filename.find(\"_\")+1:filename.find(\".\")]\n",
    "        df = df.append({'filename':filename,'text': text, 'label':label}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is preprocessed. Punctuations are removes. Digits are removed. All the contents are converted to lower case. Whitespace is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,\"text\"] = df.text.apply(lambda x : str.lower(x))\n",
    "df.loc[:,\"text\"] = df.text.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "df[\"text\"] = df['text'].str.replace('[^\\w\\s]','')\n",
    "df.loc[:,\"text\"] = df.text.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature extraction the vectorizer is provided by sklearn. It is normalized after the vectorization as many documents vary is size of content and the count of words have been normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvect = TfidfVectorizer(stop_words = 'english')\n",
    "tfdf = tfvect.fit_transform(df['text'])\n",
    "tfdfnorm = normalize(tfdf)\n",
    "X_train = tfdfnorm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of k and iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "iteration = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly 5 centroids are choosen from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = X_train[np.random.choice(X_train.shape[0],k,replace = False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializatin of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.zeros(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of KMeans algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(iteration):\n",
    "    cluster_dist_1 = -1\n",
    "    cluster_dist_2 = -1\n",
    "    cluster_dist_3 = -1\n",
    "    cluster_dist_4 = -1\n",
    "    cluster_dist_5 = -1\n",
    "    for i in range(X_train.shape[0]):\n",
    "        cluster_dist_1 = euclidean_distance(X_train[i],centroids[0])\n",
    "        cluster_dist_2 = euclidean_distance(X_train[i],centroids[1])\n",
    "        cluster_dist_3 = euclidean_distance(X_train[i],centroids[2])\n",
    "        cluster_dist_4 = euclidean_distance(X_train[i],centroids[3])\n",
    "        cluster_dist_5 = euclidean_distance(X_train[i],centroids[4])\n",
    "        min_dist = min(cluster_dist_1,cluster_dist_2,cluster_dist_3,cluster_dist_4,cluster_dist_5)\n",
    "        if(min_dist == cluster_dist_1):\n",
    "            clusters[i] = 1\n",
    "        elif(min_dist == cluster_dist_2):\n",
    "            clusters[i] = 2\n",
    "        elif(min_dist == cluster_dist_3):\n",
    "            clusters[i] = 3\n",
    "        elif(min_dist == cluster_dist_4):\n",
    "            clusters[i] = 4\n",
    "        elif(min_dist == cluster_dist_5):\n",
    "            clusters[i] = 5\n",
    "    np_c1 = []\n",
    "    np_c2 = []\n",
    "    np_c3 = []\n",
    "    np_c4 = []\n",
    "    np_c5 = []\n",
    "    \n",
    "    for i in range(clusters.shape[0]):\n",
    "        if(clusters[i] == 1):\n",
    "            np_c1.append(X_train[i])\n",
    "    for i in range(clusters.shape[0]):\n",
    "        if(clusters[i] == 2):\n",
    "            np_c2.append(X_train[i])\n",
    "    for i in range(clusters.shape[0]):\n",
    "        if(clusters[i] == 3):\n",
    "            np_c3.append(X_train[i])\n",
    "    for i in range(clusters.shape[0]):\n",
    "        if(clusters[i] == 4):\n",
    "            np_c4.append(X_train[i])\n",
    "    for i in range(clusters.shape[0]):\n",
    "        if(clusters[i] == 5):\n",
    "            np_c5.append(X_train[i])\n",
    "    np_c11 = np.array(np_c1)\n",
    "    np_c21 = np.array(np_c2)\n",
    "    np_c31 = np.array(np_c3)\n",
    "    np_c41 = np.array(np_c4)\n",
    "    np_c51 = np.array(np_c5)\n",
    "    \n",
    "    centroids[0] = np.mean(np_c11,axis=0)\n",
    "    centroids[1] = np.mean(np_c21,axis=0)\n",
    "    centroids[2] = np.mean(np_c31,axis=0)\n",
    "    centroids[3] = np.mean(np_c41,axis=0)\n",
    "    centroids[4] = np.mean(np_c51,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.iloc[:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,Y_temp_test = df['text'],df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = []\n",
    "Y_temp_test = np.array(Y_temp_test)\n",
    "for i in range(Y_temp_test.shape[0]):\n",
    "    Y_test.append(int(Y_temp_test[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing and normalizing the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdf = tfvect.transform(X_test)\n",
    "tfdfnorm = normalize(tfdf)\n",
    "X_test = tfdf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []   \n",
    "cluster_dist_1 = -1\n",
    "cluster_dist_2 = -1\n",
    "cluster_dist_3 = -1\n",
    "cluster_dist_4 = -1\n",
    "cluster_dist_5 = -1\n",
    "for i in range(X_train.shape[0]):\n",
    "    cluster_dist_1 = euclidean_distance(X_train[i],centroids[0])\n",
    "    cluster_dist_2 = euclidean_distance(X_train[i],centroids[1])\n",
    "    cluster_dist_3 = euclidean_distance(X_train[i],centroids[2])\n",
    "    cluster_dist_4 = euclidean_distance(X_train[i],centroids[3])\n",
    "    cluster_dist_5 = euclidean_distance(X_train[i],centroids[4])\n",
    "    min_dist = min(cluster_dist_1,cluster_dist_2,cluster_dist_3,cluster_dist_4,cluster_dist_5)\n",
    "    if(min_dist == cluster_dist_1):\n",
    "        predict.append(1)\n",
    "    elif(min_dist == cluster_dist_2):\n",
    "        predict.append(2)\n",
    "    elif(min_dist == cluster_dist_3):\n",
    "        predict.append(3)\n",
    "    elif(min_dist == cluster_dist_4):\n",
    "        predict.append(4)\n",
    "    elif(min_dist == cluster_dist_5):\n",
    "        predict.append(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the cluster of KMeans with the original cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}\n",
    "value = []\n",
    "for i in range(len(predict)):\n",
    "    temp =[]\n",
    "    if(predict[i] == 1):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            if(predict[j] == 1):\n",
    "                temp.append(Y_train[j])\n",
    "        result[df['filename'][i]] = (Counter(temp).most_common(1)[0][0])\n",
    "        value.append(Counter(temp).most_common(1)[0][0])\n",
    "    elif(predict[i] == 2):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            if(predict[j] == 2):\n",
    "                temp.append(Y_train[j])\n",
    "        result[df['filename'][i]] = (Counter(temp).most_common(1)[0][0])\n",
    "        value.append(Counter(temp).most_common(1)[0][0])\n",
    "    elif(predict[i] == 3):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            if(predict[j] == 3):\n",
    "                temp.append(Y_train[j])\n",
    "        result[df['filename'][i]] = (Counter(temp).most_common(1)[0][0])\n",
    "        value.append(Counter(temp).most_common(1)[0][0])\n",
    "    elif(predict[i] == 4):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            if(predict[j] == 4):\n",
    "                temp.append(Y_train[j])\n",
    "        result[df['filename'][i]] = (Counter(temp).most_common(1)[0][0])\n",
    "        value.append(Counter(temp).most_common(1)[0][0])\n",
    "    elif(predict[i] == 5):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            if(predict[j] == 5):\n",
    "                temp.append(Y_train[j])\n",
    "        result[df['filename'][i]] = (Counter(temp).most_common(1)[0][0])\n",
    "        value.append(Counter(temp).most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8510144927536232"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.mean(np.array(result) == Y_train)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'274_5.txt': '5',\n",
       " '100_1.txt': '1',\n",
       " '100_2.txt': '2',\n",
       " '100_3.txt': '3',\n",
       " '100_4.txt': '5',\n",
       " '100_5.txt': '5',\n",
       " '101_1.txt': '1',\n",
       " '101_2.txt': '1',\n",
       " '101_3.txt': '1',\n",
       " '101_4.txt': '1',\n",
       " '101_5.txt': '5',\n",
       " '102_1.txt': '1',\n",
       " '102_2.txt': '3',\n",
       " '102_3.txt': '3',\n",
       " '102_4.txt': '2',\n",
       " '102_5.txt': '3',\n",
       " '103_1.txt': '5',\n",
       " '103_2.txt': '2',\n",
       " '103_3.txt': '1',\n",
       " '103_4.txt': '3',\n",
       " '103_5.txt': '1',\n",
       " '104_1.txt': '1',\n",
       " '104_2.txt': '2',\n",
       " '104_3.txt': '5',\n",
       " '104_4.txt': '3',\n",
       " '104_5.txt': '2',\n",
       " '105_1.txt': '1',\n",
       " '105_2.txt': '2',\n",
       " '105_3.txt': '1',\n",
       " '105_4.txt': '3',\n",
       " '105_5.txt': '5',\n",
       " '106_1.txt': '3',\n",
       " '106_2.txt': '2',\n",
       " '106_3.txt': '1',\n",
       " '106_4.txt': '2',\n",
       " '106_5.txt': '5',\n",
       " '107_1.txt': '1',\n",
       " '107_2.txt': '2',\n",
       " '107_3.txt': '3',\n",
       " '200_2.txt': '2',\n",
       " '200_3.txt': '1',\n",
       " '200_4.txt': '1',\n",
       " '200_5.txt': '5',\n",
       " '201_1.txt': '1',\n",
       " '201_2.txt': '2',\n",
       " '201_3.txt': '3',\n",
       " '201_4.txt': '3',\n",
       " '201_5.txt': '1',\n",
       " '202_1.txt': '1',\n",
       " '202_2.txt': '2'}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
